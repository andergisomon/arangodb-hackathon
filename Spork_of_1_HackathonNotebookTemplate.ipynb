{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andergisomon/arangodb-hackathon/blob/dev_Ammar/Spork_of_1_HackathonNotebookTemplate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJx7LxQ-XRXq"
      },
      "source": [
        "# Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4nmLCFf3HoC"
      },
      "source": [
        "### Step 0: Package Installation & setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrF3x-kONUl_"
      },
      "source": [
        "Just install ```jedi``` first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0ZhvjqA3NLFN",
        "outputId": "0e514ca5-0fc9-4887-ad9f-7b8c3566abe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jedi\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi) (0.8.4)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.6 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n",
            "Collecting pip-tools==6.13.0\n",
            "  Downloading pip_tools-6.13.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting build (from pip-tools==6.13.0)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: click>=8 in /usr/local/lib/python3.11/dist-packages (from pip-tools==6.13.0) (8.1.8)\n",
            "Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.11/dist-packages (from pip-tools==6.13.0) (24.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pip-tools==6.13.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from pip-tools==6.13.0) (0.45.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build->pip-tools==6.13.0) (24.2)\n",
            "Collecting pyproject_hooks (from build->pip-tools==6.13.0)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: pyproject_hooks, build, pip-tools\n",
            "Successfully installed build-1.2.2.post1 pip-tools-6.13.0 pyproject_hooks-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jedi\n",
        "!pip install pip-tools==6.13.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSZTXz9INmGi"
      },
      "source": [
        "Make a ```requirements.in``` text that has ```nx-arangodb``` in it and upload to ```/content/``` in Colab. Then run the following to generate a dependency list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Eqdvab3GMMlW",
        "outputId": "4f83fce3-a7e4-4887-b9ce-9355fbcc292c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.\u001b[0m\n",
            "\u001b[32m#\u001b[0m\u001b[0m\n",
            "\u001b[32m# This file is autogenerated by pip-compile with Python 3.11\u001b[0m\u001b[0m\n",
            "\u001b[32m# by the following command:\u001b[0m\u001b[0m\n",
            "\u001b[32m#\u001b[0m\u001b[0m\n",
            "\u001b[32m#    pip-compile /content/requirements.in\u001b[0m\u001b[0m\n",
            "\u001b[32m#\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip-compile '/content/requirements.in'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0bGp0SrNYYi"
      },
      "source": [
        "Install ```nx-arangodb``` based on the generated ```requirements.txt```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j4XHePCvMfcF",
        "outputId": "691ca617-e116-4953-b2cd-e3790146a3a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nx-arangodb\n",
            "  Downloading nx_arangodb-1.3.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting networkx<=3.4,>=3.0 (from nx-arangodb)\n",
            "  Downloading networkx-3.4-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting phenolrs~=0.5 (from nx-arangodb)\n",
            "  Downloading phenolrs-0.5.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting python-arango~=8.1 (from nx-arangodb)\n",
            "  Downloading python_arango-8.1.6-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting adbnx-adapter~=5.0.5 (from nx-arangodb)\n",
            "  Downloading adbnx_adapter-5.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (2.32.3)\n",
            "Requirement already satisfied: rich>=12.5.1 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (13.9.4)\n",
            "Requirement already satisfied: setuptools>=45 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (75.1.0)\n",
            "Requirement already satisfied: numpy~=1.26 in /usr/local/lib/python3.11/dist-packages (from phenolrs~=0.5->nx-arangodb) (1.26.4)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (2.3.0)\n",
            "Requirement already satisfied: requests_toolbelt in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (1.0.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (2.10.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (8.6.1)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (24.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.7.1->python-arango~=8.1->nx-arangodb) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (0.1.2)\n",
            "Downloading nx_arangodb-1.3.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading adbnx_adapter-5.0.6-py3-none-any.whl (21 kB)\n",
            "Downloading networkx-3.4-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading phenolrs-0.5.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_arango-8.1.6-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: networkx, python-arango, phenolrs, adbnx-adapter, nx-arangodb\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed adbnx-adapter-5.0.6 networkx-3.4 nx-arangodb-1.3.0 phenolrs-0.5.9 python-arango-8.1.6\n"
          ]
        }
      ],
      "source": [
        "!pip install nx-arangodb -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install again if some dependencies are still not resolved:"
      ],
      "metadata": {
        "id": "0J48LT4U7icx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pV0dx8Ny1q64",
        "outputId": "8f6a42a9-0d31-4715-8b3c-4a9885e22a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nx-arangodb in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: networkx<=3.4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from nx-arangodb) (3.4)\n",
            "Requirement already satisfied: phenolrs~=0.5 in /usr/local/lib/python3.11/dist-packages (from nx-arangodb) (0.5.9)\n",
            "Requirement already satisfied: python-arango~=8.1 in /usr/local/lib/python3.11/dist-packages (from nx-arangodb) (8.1.6)\n",
            "Requirement already satisfied: adbnx-adapter~=5.0.5 in /usr/local/lib/python3.11/dist-packages (from nx-arangodb) (5.0.6)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (2.32.3)\n",
            "Requirement already satisfied: rich>=12.5.1 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (13.9.4)\n",
            "Requirement already satisfied: setuptools>=45 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (75.1.0)\n",
            "Requirement already satisfied: numpy~=1.26 in /usr/local/lib/python3.11/dist-packages (from phenolrs~=0.5->nx-arangodb) (1.26.4)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (2.3.0)\n",
            "Requirement already satisfied: requests_toolbelt in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (1.0.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (2.10.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (8.6.1)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (24.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.7.1->python-arango~=8.1->nx-arangodb) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# 1. Install nx-arangodb via pip\n",
        "# Github: https://github.com/arangodb/nx-arangodb\n",
        "\n",
        "!pip install nx-arangodb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL49rhZKXRXr",
        "outputId": "09c4db5d-68d6-4afc-db5e-0368d1852b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar  5 14:59:09 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "# 2. Check if you have an NVIDIA GPU\n",
        "# Note: If this returns \"command not found\", then GPU-based algorithms via cuGraph are unavailable\n",
        "\n",
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SITrT75LXRXr",
        "outputId": "c0b442a7-cf77-43b1-8229-3f378c2812b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Requirement already satisfied: nx-cugraph-cu12 in /usr/local/lib/python3.11/dist-packages (24.12.0)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (13.3.0)\n",
            "Requirement already satisfied: networkx>=3.2 in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (3.4)\n",
            "Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (1.26.4)\n",
            "Requirement already satisfied: pylibcugraph-cu12==24.12.* in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (24.12.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-curand-cu12 in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (10.3.6.82)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12 in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (11.6.3.83)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (12.5.1.3)\n",
            "Requirement already satisfied: pylibraft-cu12==24.12.* in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (24.12.0)\n",
            "Requirement already satisfied: rmm-cu12==24.12.* in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (24.12.1)\n",
            "Requirement already satisfied: cuda-python<13.0a0,<=12.6.0,>=12.0 in /usr/local/lib/python3.11/dist-packages (from pylibraft-cu12==24.12.*->pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (12.6.0)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.11/dist-packages (from rmm-cu12==24.12.*->pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (0.61.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x>=12.0.0->nx-cugraph-cu12) (0.8.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12->pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (12.5.82)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57->rmm-cu12==24.12.*->pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (0.44.0)\n"
          ]
        }
      ],
      "source": [
        "# 3. Install nx-cugraph via pip\n",
        "# Note: Only enable this installation if the step above is working!\n",
        "\n",
        "!pip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com # Requires CUDA-capable GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Yg4VdzwmXRXr",
        "outputId": "5bae9cf4-868b-4e1d-b509-74301d32511c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.5-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.41 (from langchain)\n",
            "  Downloading langchain_core-0.3.41-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.16-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.53-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.3.5-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.41-py3-none-any.whl (415 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.1/415.1 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.16-py3-none-any.whl (38 kB)\n",
            "Downloading langgraph_prebuilt-0.1.1-py3-none-any.whl (24 kB)\n",
            "Downloading langgraph_sdk-0.1.53-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langgraph-sdk, dataclasses-json, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langchain, langgraph, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.40\n",
            "    Uninstalling langchain-core-0.3.40:\n",
            "      Successfully uninstalled langchain-core-0.3.40\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.19\n",
            "    Uninstalling langchain-0.3.19:\n",
            "      Successfully uninstalled langchain-0.3.19\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.20 langchain-community-0.3.19 langchain-core-0.3.41 langgraph-0.3.5 langgraph-checkpoint-2.0.16 langgraph-prebuilt-0.1.1 langgraph-sdk-0.1.53 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.41)\n",
            "Collecting langchain-mistralai\n",
            "  Downloading langchain_mistralai-0.2.7-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (0.3.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.10.6)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from langchain-mistralai) (0.21.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langchain-mistralai) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse<1,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from langchain-mistralai) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.27.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15.1->langchain-mistralai) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2024.10.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.25.2->langchain-mistralai) (1.3.1)\n",
            "Downloading langchain_mistralai-0.2.7-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: langchain-mistralai\n",
            "Successfully installed langchain-mistralai-0.2.7\n"
          ]
        }
      ],
      "source": [
        "# 4. Install LangChain & LangGraph\n",
        "\n",
        "# !pip install --upgrade langchain langchain-community langchain-openai langgraph\n",
        "!pip install --upgrade langchain langchain-community langgraph\n",
        "!pip install -U langchain-core langchain-mistralai\n",
        "!!capture --no-stderr\n",
        "!pip install --upgrade --quiet langgraph langchain-community beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzIg-a9qXRXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553b91fb-6fe4-49d0-b323-6bccfcad0455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[14:59:45 +0000] [INFO]: NetworkX-cuGraph is available.\n",
            "INFO:nx_arangodb:NetworkX-cuGraph is available.\n"
          ]
        }
      ],
      "source": [
        "# 5. Import the required modules\n",
        "\n",
        "import networkx as nx\n",
        "import nx_arangodb as nxadb\n",
        "\n",
        "from arango import ArangoClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from langchain_community.graphs import ArangoGraph\n",
        "from arango import ArangoClient, exceptions # For AQL execution error handling\n",
        "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
        "from langchain_core.tools import tool"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key: A7K3bqlcBkmvNTXHOcA4tgZwfKczavLa"
      ],
      "metadata": {
        "id": "_-GVXhmmg-3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSmKzDgiXRXr",
        "outputId": "1c193b31-bdc8-4160-8288-6b6b628a8163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<StandardDatabase _system>\n"
          ]
        }
      ],
      "source": [
        "# 6. Connect to the ArangoDB database\n",
        "\n",
        "db = ArangoClient(hosts=\"https://fc54bbcbe286.arangodb.cloud:8529\").db(username=\"root\", password=\"OS0pStxrZG0wk7hVhzUW\", verify=True)\n",
        "\n",
        "print(db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nWdOOAqXRXs"
      },
      "source": [
        "### Step 1: Choose & prepare your dataset for NetworkX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJrfLdNxXRXs"
      },
      "source": [
        "This section will provide a template for data transformation, including placeholders for loading CSV/JSON files, defining graph schemas, and preparing data for ingestion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-tuJYczXRXs",
        "outputId": "caddc1b1-f7ab-42ae-a777-244dcbb470ca",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-05 14:59:47--  https://snap.stanford.edu/data/finefoods.txt.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 122104202 (116M) [application/x-gzip]\n",
            "Saving to: ‘finefoods.txt.gz’\n",
            "\n",
            "finefoods.txt.gz    100%[===================>] 116.45M  17.9MB/s    in 5.0s    \n",
            "\n",
            "2025-03-05 14:59:52 (23.1 MB/s) - ‘finefoods.txt.gz’ saved [122104202/122104202]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Download the dataset\n",
        "!wget https://snap.stanford.edu/data/finefoods.txt.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip finefoods.txt.gz # Unzip the dataset"
      ],
      "metadata": {
        "id": "uMnQGHIq5vwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data(file_path): # Function for parsing the dataset\n",
        "       data = []\n",
        "       current_record = {}\n",
        "       with open(file_path, 'r') as file:\n",
        "           for line in file:\n",
        "               line = line.strip()\n",
        "               #print(f\"Current line: {line}\") # Don't run print here. It will mess up Colab\n",
        "               if not line:  # Empty line indicates end of a record\n",
        "                   if current_record:\n",
        "                       data.append(current_record)\n",
        "                       current_record = {}\n",
        "               else:\n",
        "                   # Check if the delimiter is present before splitting\n",
        "                   if ': ' in line:\n",
        "                       key, value = line.split(': ', 1)\n",
        "                       current_record[key] = value\n",
        "                   else:\n",
        "                       # Handle lines without the delimiter (e.g., print or skip)\n",
        "                       print(f\"Skipping delimiterless entry in record: {line}\")\n",
        "\n",
        "                       # or continue to skip the line silently\n",
        "\n",
        "           if current_record:  # Append the last record\n",
        "               data.append(current_record)\n",
        "               print(f\"Record successfully appended: {current_record}\")\n",
        "       return data"
      ],
      "metadata": {
        "id": "-6gQrxtu6P7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!iconv -f utf-8 -t utf-8 -c finefoods.txt > finefoods_cleaned.txt #sanitize finefoods.txt, some bytes are illegal in UTF-8\n",
        "\n",
        "data = parse_data('finefoods_cleaned.txt')\n",
        "finefoods = pd.DataFrame(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoK1VE4K6V4D",
        "outputId": "2eb9ace1-6a27-4d75-a8f8-92b11bb6ae13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping delimiterless entry in record: 88 years old. ...\n",
            "Skipping delimiterless entry in record: ...creative powers b...\n",
            "Skipping delimiterless entry in record: School Princi...\n",
            "Skipping delimiterless entry in record: School Princi...\n",
            "Skipping delimiterless entry in record: I am a voracious reader/li...\n",
            "Skipping delimiterless entry in record: School Princi...\n",
            "Skipping delimiterless entry in record: ...creative powers b...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfx5vaqx2bWl",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 2. Load the dataset into a CSV\n",
        "# Reference: https://networkx.org/nx-guides/content/exploratory_notebooks/facebook_notebook.html#analysis\n",
        "# Reference: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
        "\n",
        "# mathoverflow = pd.read_csv(\n",
        "#     \"./sx-mathoverflow-a2q.txt.gz\",\n",
        "#     compression=\"gzip\",\n",
        "#     sep=\" \",\n",
        "#     names=[\"start_node\", \"end_node\"],\n",
        "# )\n",
        "\n",
        "# mathoverflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKd2Fn4bXRXs"
      },
      "source": [
        "### Step 2: Convert and Load Graph Data into NetworkX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0n4uKFmXRXs"
      },
      "source": [
        "This section will include placeholders for defining graph nodes, edges, and visualizing small test datasets before full ingestion.\n",
        "\n",
        "Although we are creating a NetworkX Graph from a Pandas Edgelist in this example, there many other ways you can load data into NetworkX.\n",
        "\n",
        "For more information about this, refer to the following documentation: https://networkx.org/documentation/stable/reference/convert.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnGqBwYO49Kq",
        "outputId": "393c87f4-cc8e-4739-e464-d4b076b92ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph with 330317 nodes and 560804 edges\n"
          ]
        }
      ],
      "source": [
        "# 1. Load the dataset a NetworkX Graph\n",
        "# Reference: https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.from_pandas_edgelist.html\n",
        "\n",
        "G = nx.from_pandas_edgelist(finefoods, \"product/productId\", \"review/userId\", edge_attr=[\"product/productId\", \"review/profileName\", \"review/helpfulness\", \"review/score\", \"review/summary\", \"review/text\"])\n",
        "\n",
        "print(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZSE_8AmdZvK"
      },
      "source": [
        "Probably shouldn't run this visualize graph block, it somehow hogs up to 16GB of your system RAM for some reason"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nCVADSnXRXt"
      },
      "outputs": [],
      "source": [
        "# 3. Visualize the Graph\n",
        "# Reference: https://networkx.org/nx-guides/content/exploratory_notebooks/facebook_notebook.html#visualizing-the-graph\n",
        "\n",
        "#plot_options = {\"node_size\": 3, \"with_labels\": False, \"width\": 0.15}\n",
        "#pos = nx.spring_layout(G, iterations=3, seed=1721)\n",
        "#fig, ax = plt.subplots(figsize=(15, 9))\n",
        "#nx.draw_networkx(G, pos=pos, ax=ax, **plot_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDwAaBcsXRXt"
      },
      "source": [
        "### Step 3: Persist the Graph in ArangoDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnuRm6XsXRXt"
      },
      "source": [
        "This section will include pre-written code templates for interacting with ArangoDB, making it easier for participants to insert and query their data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3vwBKpZXRXt"
      },
      "source": [
        "For this new [nxadb.Graph()](https://nx-arangodb.readthedocs.io/en/latest/classes/graph.html) object, we will pass `name=\"Facebook\"`, `db=db`, and `incoming_graph_data=G_nx`. This last parameter allows us to load the NetworkX Graph directly into ArangoDB:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283,
          "referenced_widgets": [
            "f35b46c3945d45de8df36bd217d13224",
            "f7a04313c37c400eb8958ce9a56319d3",
            "e947e5e223574388bb94baf09134e02a",
            "2707febcf3164d1f9aa31893f80bf45a"
          ]
        },
        "id": "bO_cNX90XRXt",
        "outputId": "3d5f3447-c79a-4e5b-a225-3113184c4227",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[15:00:12 +0000] [INFO]: Overwriting graph 'Finefoods'\n",
            "INFO:nx_arangodb:Overwriting graph 'Finefoods'\n",
            "[15:00:12 +0000] [INFO]: Graph 'Finefoods' exists.\n",
            "INFO:nx_arangodb:Graph 'Finefoods' exists.\n",
            "[15:00:12 +0000] [INFO]: Default node type set to 'Finefoods_node'\n",
            "INFO:nx_arangodb:Default node type set to 'Finefoods_node'\n",
            "[2025/03/05 15:00:12 +0000] [281] [INFO] - adbnx_adapter: Instantiated ADBNX_Adapter with database '_system'\n",
            "INFO:adbnx_adapter:Instantiated ADBNX_Adapter with database '_system'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f35b46c3945d45de8df36bd217d13224"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e947e5e223574388bb94baf09134e02a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2025/03/05 15:01:37 +0000] [281] [INFO] - adbnx_adapter: Created ArangoDB 'Finefoods' Graph\n",
            "INFO:adbnx_adapter:Created ArangoDB 'Finefoods' Graph\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph named 'Finefoods' with 330317 nodes and 510001 edges\n"
          ]
        }
      ],
      "source": [
        "# 1. Load the NetworkX Graph into ArangoDB\n",
        "# Reference: https://github.com/arangodb/nx-arangodb?tab=readme-ov-file#can-i-create-an-arangodb-graph-from-an-existing-networkx-graph\n",
        "\n",
        "G_adb = nxadb.Graph(\n",
        "    name=\"Finefoods\",\n",
        "    db=db,\n",
        "    incoming_graph_data=G,\n",
        "    write_batch_size=10000, # feel free to modify\n",
        "    overwrite_graph=True,\n",
        ")\n",
        "\n",
        "print(G_adb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTVOtFUyXRXt"
      },
      "source": [
        "Great! We've created our first [nxadb.Graph()](https://nx-arangodb.readthedocs.io/en/latest/classes/graph.html) object.\n",
        "\n",
        "Note that there are [4 Graph Types](https://networkx.org/documentation/stable/reference/classes/index.html) in NetworkX, which are also supported as [nxadb Graphs](https://nx-arangodb.readthedocs.io/en/latest/classes/index.html):\n",
        "\n",
        "1. `nxadb.Graph`: This class implements an undirected graph. It ignores multiple edges between two nodes. It does allow self-loop edges between a node and itself.\n",
        "2. `nxadb.DiGraph`: Directed graphs, that is, graphs with directed edges. Provides operations common to directed graphs, (a subclass of Graph).\n",
        "3. `nxadb.MultiGraph`: A flexible graph class that allows multiple undirected edges between pairs of nodes. The additional flexibility leads to some degradation in performance, though usually not significant.\n",
        "4. `nxadb.MultiDiGraph`: A directed version of a MultiGraph.\n",
        "\n",
        "Let's continue with the same Graph for now, and re-instantiate the `G_adb` object to showcase the persistence factor!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3ThlpALI5G9",
        "outputId": "b4d1a64e-1a45-4ea0-97a2-96d40fab005d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[15:01:38 +0000] [INFO]: Graph 'Finefoods' exists.\n",
            "INFO:nx_arangodb:Graph 'Finefoods' exists.\n",
            "[15:01:39 +0000] [INFO]: Default node type set to 'Finefoods_node'\n",
            "INFO:nx_arangodb:Default node type set to 'Finefoods_node'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph named 'Finefoods' with 330317 nodes and 520804 edges\n"
          ]
        }
      ],
      "source": [
        "# 2. Re-connect to the same Graph\n",
        "\n",
        "G_adb = nxadb.Graph(name=\"Finefoods\", db=db)\n",
        "\n",
        "print(G_adb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zGN1FriXRXu"
      },
      "source": [
        "Feel free to take a moment to view the created graph in ArangoDB:\n",
        "\n",
        "1. Log into your deployment with username `root` and your designated password.\n",
        "2. Select your database (most likely `_system`)\n",
        "3. Navigate to the `GRAPHS` tab (selectable on the left-hand side after login)\n",
        "4. Click on your graph (in this example, it would be called `Facebook`)\n",
        "5. Experiment with the visualizer options to learn more about your Graph\n",
        "\n",
        "For example, here's a quick sample of our `Facebook` graph:\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"attachment:54d53cbe-c3b4-407a-9acf-b141c561f14f.png\" style=\"height: 400px;\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyJQSd8cXRX0"
      },
      "source": [
        "### Step 4: Build the Agentic App with LangChain & LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaNLPLg9XRX0"
      },
      "source": [
        "This section will provide pre-built agent templates, guiding participants on how to dynamically select AQL vs. NetworkX/cuGraph execution paths based on user input.\n",
        "\n",
        "Reference:\n",
        "- https://www.langchain.com/\n",
        "- https://www.langchain.com/langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHpIFqeYXRX0"
      },
      "outputs": [],
      "source": [
        "# 1. Create the ArangoGraph LangChain wrapper\n",
        "# Reference: https://api.python.langchain.com/en/latest/graphs/langchain_community.graphs.arangodb_graph.ArangoGraph.html\n",
        "\n",
        "arango_graph = ArangoGraph(db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4stM5TzT-LSX"
      },
      "source": [
        "We're gonna use models from MistralAI:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vCZvx_xfHX5"
      },
      "source": [
        "API Key (Do not delete this): ```A7K3bqlcBkmvNTXHOcA4tgZwfKczavLa```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT49bZ8jIGHh",
        "outputId": "71c02a4a-213b-4acf-a81a-c54beeef2caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "MISTRAL_API_KEY = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Important to prevent to many requests from being sent to MistralAI\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.1,  # <-- Super slow! We can only make a request once every 10 seconds!!\n",
        "    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,\n",
        "    max_bucket_size=3,  # Controls the maximum burst size.\n",
        ")"
      ],
      "metadata": {
        "id": "HPXVsHqt9ldh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvRBvrQ3JHbx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"MISTRAL_API_KEY\"] = MISTRAL_API_KEY\n",
        "\n",
        "llm = ChatMistralAI(\n",
        "    model = \"mistral-small-latest\",\n",
        "    temperature = 0.5,\n",
        "    max_tokens = 1000,\n",
        "    rate_limiter=rate_limiter,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing time module\n",
        "import time"
      ],
      "metadata": {
        "id": "ec3PiaZQAMdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsJQ3t_HXRX1"
      },
      "outputs": [],
      "source": [
        "# 4. Define the Text to AQL Tool\n",
        "# Reference: https://python.langchain.com/docs/integrations/graphs/arangodb/\n",
        "# Reference: https://python.langchain.com/api_reference/community/chains/langchain_community.chains.graph_qa.arangodb.ArangoGraphQAChain.html\n",
        "# Note: It is encouraged to experiment and improve this section! This is just a placeholder:\n",
        "@tool\n",
        "def text_to_aql_to_text(query: str):\n",
        "    \"\"\"This tool is available to invoke the\n",
        "    ArangoGraphQAChain object, which enables you to\n",
        "    translate a Natural Language Query into AQL, execute\n",
        "    the query, and translate the result back into Natural Language.\n",
        "    \"\"\"\n",
        "\n",
        "    llm = ChatMistralAI(model = \"mistral-small-latest\", temperature = 0.5, max_tokens = 1000, rate_limiter=rate_limiter,)\n",
        "\n",
        "    time.sleep(0.1)\n",
        "\n",
        "    chain = ArangoGraphQAChain.from_llm(\n",
        "    \tllm=llm,\n",
        "    \tgraph=arango_graph,\n",
        "    \tverbose=True,\n",
        "      allow_dangerous_requests=True,\n",
        "      max_aql_generation_attempts = 4\n",
        "    )\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    attempt = 1\n",
        "    MAX_ATTEMPTS = 5\n",
        "\n",
        "    for attempt in range(attempt, MAX_ATTEMPTS):\n",
        "        time.sleep(0.1)\n",
        "        try:\n",
        "            result = chain.invoke(query)\n",
        "        except exceptions.ArangoServerError as e:\n",
        "            attempt = attempt + 1\n",
        "            print(\"Attempt number\", attempt)\n",
        "            print(f\"AQL EXEC ERROR: {e}\")\n",
        "            return f\"AQL EXEC ERROR: {e}\"\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Process_logs_list.append(str(result[\"result\"]) + \"\\n\") # For display in gradio process logs\n",
        "\n",
        "    return str(result[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05kgQHvWXRX1"
      },
      "outputs": [],
      "source": [
        "# 5. Define the Text to NetworkX/cuGraph Tool\n",
        "# Note: It is encouraged to experiment and improve this section! This is just a placeholder:\n",
        "\n",
        "@tool\n",
        "def text_to_nx_algorithm_to_text(query):\n",
        "    \"\"\"This tool is available to invoke a NetworkX Algorithm on\n",
        "    the ArangoDB Graph. You are responsible for accepting the\n",
        "    Natural Language Query, establishing which algorithm needs to\n",
        "    be executed, executing the algorithm, and translating the results back\n",
        "    to Natural Language, with respect to the original query.\n",
        "\n",
        "    If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n",
        "    this tool.\n",
        "    When you generate the Python code, only output the executable python code. Do not add the word \"python\" or anything\n",
        "    that might cause the code to return a syntax error.\n",
        "    \"\"\"\n",
        "    ########################\n",
        "\n",
        "    llm_invoke_prompt = f\"\"\"\n",
        "    I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
        "\n",
        "    I have the following graph analysis query: {query}.\n",
        "\n",
        "    Generate the Python Code required to answer the query using the `G_adb` object.\n",
        "\n",
        "    Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n",
        "\n",
        "    Only assume that networkx is installed, and other base python dependencies.\n",
        "\n",
        "    Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n",
        "\n",
        "    Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n",
        "\n",
        "    Make sure that `FINAL_RESULT` stores a short & concise answer. Avoid setting this variable to a long sequence.\n",
        "    Your code:\n",
        "    \"\"\"\n",
        "    global Process_logs_list\n",
        "\n",
        "    llm = ChatMistralAI(model = \"mistral-small-latest\", temperature = 0.5, max_tokens = 1000, rate_limiter=rate_limiter,)\n",
        "\n",
        "    print(\"1) Generating NetworkX code\")\n",
        "    # Process_logs_list.append(\"1) Generating NetworkX code\\n\") # For display in gradio process logs\n",
        "\n",
        "    text_to_nx = llm.invoke(llm_invoke_prompt).content\n",
        "    time.sleep(0.1)\n",
        "    text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n",
        "\n",
        "    print('-'*10)\n",
        "    print(text_to_nx_cleaned)\n",
        "    print('-'*10)\n",
        "\n",
        "    # Process_logs_list.append(\"----------\\n\") # For display in gradio process logs\n",
        "    # Process_logs_list.append(text_to_nx_cleaned)\n",
        "    # Process_logs_list.append(\"----------\\n\")\n",
        "\n",
        "    ######################\n",
        "\n",
        "    print(\"\\n2) Executing NetworkX code\")\n",
        "    # Process_logs_list.append(\"\\n2) Executing NetworkX code\\n\")\n",
        "\n",
        "    global_vars = {\"G_adb\": G_adb, \"nx\": nx}\n",
        "    local_vars = {}\n",
        "\n",
        "    # try:\n",
        "    #     exec(text_to_nx_cleaned, global_vars, local_vars)\n",
        "    #     text_to_nx_final = text_to_nx\n",
        "    # except Exception as e:\n",
        "    #     print(f\"EXEC ERROR: {e}\")\n",
        "    #     return f\"EXEC ERROR: {e}\"\n",
        "    # TODO: Consider experimenting with a code corrector!\n",
        "    attempt = 1\n",
        "    MAX_ATTEMPTS = 5\n",
        "\n",
        "    for attempt in range(attempt, MAX_ATTEMPTS):\n",
        "        time.sleep(0.1)\n",
        "        try:\n",
        "            text_to_nx_final = llm.invoke(llm_invoke_prompt).content\n",
        "            text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx_final, flags=re.MULTILINE).strip()\n",
        "\n",
        "            print(\"Attempt number\", attempt)\n",
        "            # Attempt_number = \"Attempt number \" + str(attempt) + \"\\n\"\n",
        "            # Process_logs_list.append(Attempt_number)\n",
        "\n",
        "            exec(text_to_nx_cleaned, global_vars, local_vars)\n",
        "        except BaseException as e:\n",
        "            attempt = attempt + 1\n",
        "            print(\"Attempt number\", attempt)\n",
        "            print(f\"EXEC ERROR: {e}\")\n",
        "\n",
        "            # Attempt_number = \"Attempt number \" + str(attempt) + \"\\n\"\n",
        "            # Process_logs_list.append(Attempt_number)\n",
        "\n",
        "            # EXEC_ERROR = \"EXEC ERROR: \" + str(e) + \"\\n\" # <-- Does this even work\n",
        "            # Process_logs_list.append(EXEC_ERROR)\n",
        "\n",
        "            return f\"EXEC ERROR: {e}\"\n",
        "            continue\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print('-'*10)\n",
        "    FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n",
        "    print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n",
        "    print('-'*10)\n",
        "\n",
        "    # FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n",
        "\n",
        "    # Process_logs_list.append(\"----------\\n\") # For display in gradio process logs\n",
        "    # Process_logs_list.append(\"FINAL_RESULT: \" + str(FINAL_RESULT))\n",
        "    # Process_logs_list.append(\"----------\\n\")\n",
        "\n",
        "    ######################\n",
        "\n",
        "    print(\"3) Formulating final answer\")\n",
        "    # Process_logs_list.append(\"3) Formulating final answer\\n\")\n",
        "\n",
        "    time.sleep(0.1)\n",
        "    nx_to_text = llm.invoke(f\"\"\"\n",
        "        I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
        "\n",
        "        I have the following graph analysis query: {query}.\n",
        "\n",
        "        I have executed the following python code to help me answer my query:\n",
        "\n",
        "        ---\n",
        "        {text_to_nx_final}\n",
        "        ---\n",
        "\n",
        "        The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n",
        "\n",
        "        Based on my original Query and FINAL_RESULT, generate a short and concise response to\n",
        "        answer my query.\n",
        "\n",
        "        Your response:\n",
        "    \"\"\").content\n",
        "\n",
        "    # Process_logs_list.append(nx_to_text + \"\\n\") # For display in gradio process logs\n",
        "\n",
        "    return nx_to_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7zPypxIXRX1"
      },
      "outputs": [],
      "source": [
        "# 7. Experiment with example queries\n",
        "# Note: Some may work, some may not!\n",
        "\n",
        "# query = \"What is your favourite fruit?\"\n",
        "# query = \"Are there isolated nodes?\"\n",
        "# query = \"Which node has the highest betweenness centrality score? Use a k value of 10\"\n",
        "# query = \"Who is connected to Node 0?\"\n",
        "# query = \"What is the shortest path from Node 0 to Node 1?\"\n",
        "# query = \"Update node 0 to have attribute foo=bar\"\n",
        "# query = \"Is the graph fully connected?\"\n",
        "# query = \"What is the average degree of nodes?\"\n",
        "# query = \"Are there nodes that, if removed, would fragment the network?\"\n",
        "# query = \"Which users are outliers in terms of connections?\"\n",
        "# query = \"Which nodes are the most connected?\"\n",
        "# query = \"How strongly connected is the network? Used connected components\"\n",
        "# query = \"Fetch the highest pagerank node and value\"\n",
        "# query = \"Who is the most influential node?\"\n",
        "# query = \"Who is the most popular person in the Graph? Explain why\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step AA: Create chat history & retrieve tool\n"
      ],
      "metadata": {
        "id": "k6tikTOs4p7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_tool_calling_agent, tool\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "5fhyLrTIBxAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation based on: https://python.langchain.com/docs/how_to/qa_chat_history_how_to/\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install -qU \"langchain[mistralai]\"\n",
        "!pip install -qU langchain-mistralai\n",
        "!pip install -qU langchain-core"
      ],
      "metadata": {
        "id": "VaNcs0jca7n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "llm = init_chat_model(\"mistral-small-latest\", model_provider=\"mistralai\",temperature = 0.5, max_tokens = 1000, rate_limiter=rate_limiter,)\n",
        "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "biPlnioAaDpW",
        "outputId": "02a4aec7-ced2-4c29-ced4-2ad401fa45d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This looks at previous queries to see if there has already been any answers to the current one.\n",
        "@tool(response_format=\"content_and_artifact\")\n",
        "def retrieve(query: str):\n",
        "    \"\"\"Retrieve information related to a query. If there no information is found,\n",
        "    ask the agent to use other tools instead.\n",
        "    \"\"\"\n",
        "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
        "    serialized = \"\\n\\n\".join(\n",
        "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    return serialized, retrieved_docs"
      ],
      "metadata": {
        "id": "9kB0tqN8bktu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step BB: Define the name of description of all tools\n",
        "Trying to get the LLM to know how to call tools"
      ],
      "metadata": {
        "id": "hh8FdF3UpDjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain.tools import Tool\n",
        "from langchain_core.tools.structured import StructuredTool"
      ],
      "metadata": {
        "id": "oKAJVEOx0qrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_aql_to_text.name = \"Text_to_AQL_to_Text\"\n",
        "text_to_nx_algorithm_to_text.name = \"Text_to_NetworkX_cuGraph\"\n",
        "retrieve.name = \"Retrieve\"\n",
        "\n",
        "text_to_aql_to_text.description = \"This tool is available to invoke the ArangoGraphQAChain object, which enables you to translate a Natural Language Query into AQL, execute the query, and translate the result back into Natural Language.\"\n",
        "text_to_nx_algorithm_to_text.description = \"This tool is available to invoke a NetworkX Algorithm on the ArangoDB Graph. You are responsible for accepting the Natural Language Query, establishing which algorithm needs to be executed, executing the algorithm, and translating the results back to Natural Language, with respect to the original query. If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use this tool. When you generate the Python code, only output the executable python code. Do not add the word 'python' or anything that might cause the code to return a syntax error.\"\n",
        "retrieve.description = \"Retrieve information related to a query. If there no information is found, ask the agent to use other tools instead.\"\n",
        "\n",
        "tools = [retrieve, text_to_aql_to_text, text_to_nx_algorithm_to_text]"
      ],
      "metadata": {
        "id": "trBY3hCm-9nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step CC: Create the agent"
      ],
      "metadata": {
        "id": "-s557sEo0ENV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant. If one tool doesn't work, use another tool.\"),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{messages}\"),                # changes {input} -> {messages}\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "    ]\n",
        ") # TODO: prompt template needs to specify generated plot output\n",
        "\n",
        "agent = create_react_agent(llm,tools=tools, prompt=prompt, checkpointer=memory)"
      ],
      "metadata": {
        "id": "3yxy-wlHcixo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step DD: Define file storing functions"
      ],
      "metadata": {
        "id": "aWRixZRhR8N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"10\"}}"
      ],
      "metadata": {
        "id": "v7_iOVzOccX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠿⠛⠛⠛⠛⠿⣿⣿⣿⣿⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠛⠉⠁⠀⠀⠀⠀⠀⠀⠀⠉⠻⣿⣿⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢿⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⣿⠋⠈⠀⠀⠀⠀⠐⠺⣖⢄⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⡏⢀⡆⠀⠀⠀⢋⣭⣽⡚⢮⣲⠆⠀⠀⠀⠀⠀⠀⢹⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⡇⡼⠀⠀⠀⠀⠈⠻⣅⣨⠇⠈⠀⠰⣀⣀⣀⡀⠀⢸⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⡇⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣟⢷⣶⠶⣃⢀⣿⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⡅⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⠀⠈⠓⠚⢸⣿⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⢀⡠⠀⡄⣀⠀⠀⠀⢻⠀⠀⠀⣠⣿⣿⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠐⠉⠀⠀⠙⠉⠀⠠⡶⣸⠁⠀⣠⣿⣿⣿⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⣿⣦⡆⠀⠐⠒⠢⢤⣀⡰⠁⠇⠈⠘⢶⣿⣿⣿⣿⣿⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠠⣄⣉⣙⡉⠓⢀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿\n",
        "# ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣤⣀⣀⠀⣀⣠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿"
      ],
      "metadata": {
        "id": "dAuXgqjk1IM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nuT53SLbF_8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from langchain_core.messages import AIMessage, HumanMessage, RemoveMessage\n",
        "\"\"\" Can Remove This\n",
        "\n",
        "# Don't really need to define a global dataframe, qHistory lists are enough\n",
        "# query_history = pd.DataFrame(columns=[\"HumanMessage\", \"AIMessage\"]) #,\"GeneratedPlotPath\"])\n",
        "# query_history = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}) # FOR TESTING\n",
        "\"\"\"\n",
        "\n",
        "qHistory_AI = []\n",
        "qHistory_Human = []\n",
        "\n",
        "# Function for saving current queries and answers into a downloadable .csv file\n",
        "def save_curr_query_history():\n",
        "    # global query_history\n",
        "\n",
        "    # Save the current queries into a dataframe\n",
        "    # query_history = pd.DataFrame({\"HumanMessage\": qHistory_Human,\n",
        "    #                                \"AIMessage\": qHistory_AI,})\n",
        "                                # \"GeneratedPlotPath\": Generated_plot_list})\n",
        "\n",
        "    print(\"Saved current query history as a dataframe!\")\n",
        "\n",
        "    # Save the current queries into a csv file\n",
        "    query_history.to_csv('/content/user_query_history.csv', index = False)\n",
        "    print(\"Saved current query history into a csv file!\")\n",
        "\n",
        "# Function for providing past queries and answers to the LLM as context\n",
        "# Run if user chooses to upload past history, prolly don't need this function if user has to upload file before executing gradio\n",
        "def load_past_query_history():\n",
        "    global qHistory_AI\n",
        "    global qHistory_Human\n",
        "\n",
        "    my_file = Path(\"/content/user_query_history.csv\")\n",
        "    if my_file.is_file():\n",
        "        # File exists, so load it into qHistory_Human and QHistory_AI\n",
        "        print(\"File exists\")\n",
        "\n",
        "        df = pd.read_csv('/content/user_query_history.csv')\n",
        "        qHistory_Human = df['A'].tolist()   # Obviously change col to HumanMessage\n",
        "        qHistory_AI = df['B'].tolist()      # Obviously change col to AIMessage\n",
        "\n",
        "        print(qHistory_Human)\n",
        "        print(qHistory_AI)\n",
        "\n",
        "        # Now load them into the memory\n",
        "        # It's expected that len(qHistory_Human) == len(qHistory_AI)\n",
        "        # So each query is paired with an answer\n",
        "        for i in range(len(qHistory_Human)):\n",
        "            _ = agent.update_state(config, {\"messages\": [HumanMessage(qHistory_Human[i])]})\n",
        "            _ = agent.update_state(config, {\"messages\": [AIMessage(qHistory_AI[i])]})\n",
        "\n",
        "    else:\n",
        "        print(\"File Don't Exist\")\n",
        "        # File doesn't exist\n"
      ],
      "metadata": {
        "id": "ZMhYmQ-yR_df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step EE: Define functions for Gradio"
      ],
      "metadata": {
        "id": "KF0wA_kGG0Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Important function for gradio\n",
        "# https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html for chat history\n",
        "# As of now, just run it before gradio, so user must upload first before executing gradio\n",
        "\n",
        "# if lets say there's a button, then remove this, and run when button pressed\n",
        "# load_past_query_history()\n",
        "\n",
        "Generated_plot_list = [] # List for images generated by the AI\n",
        "\n",
        "Process_logs_list = []\n",
        "\n",
        "max_tokens = 1000\n",
        "\n",
        "# Function for trimming the number of messages (Kinda shoddy but works)\n",
        "def Check_N_Trim():\n",
        "    currMessages = agent.get_state(config).values[\"messages\"]\n",
        "\n",
        "    # First message (index 0) is definitely HumanMessage (Earliest query)\n",
        "    # Remove the earliest query and all the associated ToolMessage or AIMessage\n",
        "    agent.update_state(config, {\"messages\": RemoveMessage(id=currMessages[0].id)})\n",
        "\n",
        "    # For a valid list, the first message should be HumanMessage.\n",
        "    # So remove all the responses until the next query (HumanMessage)\n",
        "    while (agent.get_state(config).values[\"messages\"][0].type != \"human\"):\n",
        "        currMessages = agent.get_state(config).values[\"messages\"]\n",
        "        agent.update_state(config, {\"messages\": RemoveMessage(id=currMessages[0].id)})\n",
        "\n",
        "    return \"Finished trimming\"\n",
        "\n",
        "# def update_process_logs():\n",
        "#   global Process_logs_list\n",
        "#   return \" \".join(Process_logs_list)\n",
        "\n",
        "def ReviewBot(query, history):\n",
        "    global check_final_state\n",
        "    #global query_history\n",
        "    global qHistory_AI\n",
        "    global qHistory_Human\n",
        "    #global Generated_plot_list\n",
        "\n",
        "    # Remove the oldest query if the number of AIMessages + HumanMessages + ToolMessages exceeds 100\n",
        "    currMessages = agent.get_state(config).values[\"messages\"]\n",
        "    while(len(currMessages) > 100):\n",
        "      Check_N_Trim()\n",
        "\n",
        "    # Generate the output\n",
        "    final_state = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]}, config=config)\n",
        "\n",
        "    # Store the answers to the query for storing in a .csv file later\n",
        "    qHistory_Human.append(query)\n",
        "    qHistory_AI.append(final_state[\"messages\"][-1].content)\n",
        "\n",
        "    if (query == \"Meow\"): # If an graph/chart/plot is generated, add onto Generated_plot_list\n",
        "      print(\"Do nothing\")\n",
        "      #Generated_plot_list.append(the generated_graph or whatever)\n",
        "\n",
        "    # Instantly output the process logs\n",
        "    display_logs = \"Empty\"#update_process_logs()\n",
        "\n",
        "    # First return is for chat output; Second return is for the chart(s); Third is for process logs\n",
        "    return final_state[\"messages\"][-1].content, Generated_plot_list, display_logs #final_state[\"output\"], Generated_plot_list, display_logs"
      ],
      "metadata": {
        "id": "_Y-mIhS0trIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step FF: Create user frontend with Gradio\n",
        "Gradio app"
      ],
      "metadata": {
        "id": "BnNsyFPTpVrU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccmx8i6kXRX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e1bf3b-1716-4cf3-9cb0-bb64c2c8f0fa",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.20.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.20.0-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.20.0 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.0 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "# 8. (Optional) Set up UI via Gradio\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install gradio\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZnpjEBwXRX2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d17ea39c-db3e-4bd6-ce11-fe7fb36e7291",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://62b4fc63c1cf03106a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://62b4fc63c1cf03106a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ArangoGraphQAChain chain...\u001b[0m\n",
            "AQL Query (1):\u001b[32;1m\u001b[1;3mWITH Finefoods_node_to_Finefoods_node\n",
            "FOR vertex IN Finefoods_node\n",
            "  FILTER vertex._id == 'Finefoods_node/0'\n",
            "  FOR edge IN 1..1 OUTBOUND vertex Finefoods_node_to_Finefoods_node\n",
            "    RETURN edge._to\n",
            "\u001b[0m\n",
            "AQL Result:\n",
            "\u001b[32;1m\u001b[1;3m[None]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1) Generating NetworkX code\n",
            "----------\n",
            "import networkx as nx\n",
            "\n",
            "# Assuming G_adb is already defined and loaded with the given schema\n",
            "\n",
            "# Step 1: Identify the node with the key '0' in the 'Finefoods_node' collection\n",
            "node_id = 'Finefoods_node/0'\n",
            "\n",
            "# Step 2: Use the neighbors function from NetworkX to find all nodes connected to node '0'\n",
            "neighbors = list(G_adb.neighbors(node_id))\n",
            "\n",
            "# Step 3: Extract the keys of the neighboring nodes\n",
            "neighbor_keys = [G_adb.nodes[neighbor]['_key'] for neighbor in neighbors]\n",
            "\n",
            "# Step 4: Format the result as a concise answer\n",
            "FINAL_RESULT = f\"Node 0 is connected to nodes: {', '.join(neighbor_keys)}\"\n",
            "----------\n",
            "\n",
            "2) Executing NetworkX code\n",
            "Attempt number 1\n",
            "----------\n",
            "FINAL_RESULT: Finefoods_node/1\n",
            "----------\n",
            "3) Formulating final answer\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ArangoGraphQAChain chain...\u001b[0m\n",
            "AQL Query (1):\u001b[32;1m\u001b[1;3mWITH Finefoods_node_to_Finefoods_node\n",
            "FOR vertex IN Finefoods_node_to_Finefoods_node\n",
            "  FILTER vertex._from == 'Finefoods_node/0' OR vertex._to == 'Finefoods_node/0'\n",
            "  RETURN vertex\n",
            "\u001b[0m\n",
            "AQL Result:\n",
            "\u001b[32;1m\u001b[1;3m[{'_key': '0', '_id': 'Finefoods_node_to_Finefoods_node/0', '_from': 'Finefoods_node/0', '_to': 'Finefoods_node/1', '_rev': '_jUE3W3W-Bc', 'product/productId': 'B001E4KFG0', 'review/profileName': 'delmartian', 'review/helpfulness': '1/1', 'review/score': '5.0', 'review/summary': 'Good Quality Dog Food', 'review/text': 'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1) Generating NetworkX code\n",
            "----------\n",
            "import networkx as nx\n",
            "\n",
            "# Assuming G_adb is already defined and loaded with the given schema\n",
            "\n",
            "# Step 1: Identify the node with _key '0' in the 'Finefoods_node' collection\n",
            "node_id = 'Finefoods_node/0'\n",
            "\n",
            "# Step 2: Use the neighbors function to find all nodes connected to node_id\n",
            "neighbors = list(G_adb.neighbors(node_id))\n",
            "\n",
            "# Step 3: Extract the _key values of the connected nodes\n",
            "connected_keys = [G_adb.nodes[neighbor]['_key'] for neighbor in neighbors]\n",
            "\n",
            "# Step 4: Format the result\n",
            "FINAL_RESULT = ', '.join(connected_keys)\n",
            "----------\n",
            "\n",
            "2) Executing NetworkX code\n",
            "Attempt number 1\n",
            "----------\n",
            "FINAL_RESULT: 1\n",
            "----------\n",
            "3) Formulating final answer\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://62b4fc63c1cf03106a.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "with gr.Blocks() as demo:\n",
        "  with gr.Row():\n",
        "\n",
        "    with gr.Column(scale=1): # Column for charts/graph and such\n",
        "      SaveCurrHistory = gr.Button(\"Save Current Chat History\") # All it does is convert the current lists into a dataframe\n",
        "      SaveCurrHistory.click(save_curr_query_history)\n",
        "\n",
        "      plots_gallery = gr.Gallery(\n",
        "                   label=\"Graphs/Charts\",\n",
        "                   show_label=False,\n",
        "                   elem_id=\"gallery\",\n",
        "                   columns = [1],\n",
        "                   object_fit=\"contain\",\n",
        "                   height=\"auto\"\n",
        "                   )\n",
        "\n",
        "      with gr.Accordion(\"Process Logs\"): # For process logs\n",
        "        process_logs = gr.Textbox(value = update_process_logs, every = 0.1, show_label = False)\n",
        "\n",
        "    with gr.Column(scale=5): # Column for the chatbot\n",
        "      gr.ChatInterface(fn=ReviewBot,\n",
        "                          type=\"messages\",\n",
        "                          # Gotta change these example prompts\n",
        "                          examples=[\"What is the significance of the imaginary number, i, in Schrodinger's equation?\",\n",
        "                                    \"Find the 20th number in the following sequence: 1, 4, 27, 256, ...\",\n",
        "                                    \"Provide a short explanation and derivation for the surface area of a sphere.\"],\n",
        "                          title=\"Unbiased Reviewer\",\n",
        "                          additional_outputs= [plots_gallery, process_logs], # Instantly updates both the image generated and process logs\n",
        "                          )\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"What is your favourite fruit?\"\n",
        "# query = \"Are there isolated nodes?\"\n",
        "# query = \"Which node has the highest betweenness centrality score? Use a k value of 10\"\n",
        "# query = \"Who is connected to Node 0?\"\n",
        "# query = \"What is the shortest path from Node 0 to Node 1?\"\n",
        "# query = \"Update node 0 to have attribute foo=bar\"\n",
        "# query = \"Is the graph fully connected?\"\n",
        "# query = \"What is the average degree of nodes?\"\n",
        "# query = \"Are there nodes that, if removed, would fragment the network?\"\n",
        "# query = \"Which users are outliers in terms of connections?\"\n",
        "# query = \"Which nodes are the most connected?\"\n",
        "# query = \"How strongly connected is the network? Used connected components\"\n",
        "# query = \"Fetch the highest pagerank node and value\"\n",
        "# query = \"Who is the most influential node?\"\n",
        "# query = \"Who is the most popular person in the Graph? Explain why\""
      ],
      "metadata": {
        "id": "F0WPmAdq3okq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_store)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xIEFtfEym6V",
        "outputId": "721d4d2c-2b1a-4048-de11-ed00b9d2bcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<langchain_core.vectorstores.in_memory.InMemoryVectorStore object at 0x78b08ead1ad0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = agent.get_state(config).values[\"messages\"]\n",
        "# print(chat_history)\n",
        "\n",
        "for message in chat_history:\n",
        "     message.pretty_print()\n",
        "     #print(message.content)\n",
        "    #  print(chat_history.message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jENjKlkPetey",
        "outputId": "e14a54a9-0d44-48bc-b685-9039145d9670",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Is the graph fully connected?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Text_to_NetworkX_cuGraph (hoIO2R0hS)\n",
            " Call ID: hoIO2R0hS\n",
            "  Args:\n",
            "    query: Is the graph fully connected?\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: Text_to_NetworkX_cuGraph\n",
            "\n",
            "The graph `G_adb` is not fully connected.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The graph `G_adb` is not fully connected.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "How strongly connected is the network? Used connected components\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Text_to_NetworkX_cuGraph (zUXuZWvEB)\n",
            " Call ID: zUXuZWvEB\n",
            "  Args:\n",
            "    query: How strongly connected is the network? Used connected components\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: Text_to_NetworkX_cuGraph\n",
            "\n",
            "The network is weakly connected. There are multiple connected components, and the largest component does not encompass a significant proportion of the total nodes, indicating a lack of strong overall connectivity.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The network is weakly connected. There are multiple connected components, and the largest component does not encompass a significant proportion of the total nodes, indicating a lack of strong overall connectivity.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Who is connected to Node 0?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Text_to_NetworkX_cuGraph (iyQMUrzq8)\n",
            " Call ID: iyQMUrzq8\n",
            "  Args:\n",
            "    query: Who is connected to Node 0?\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: Text_to_NetworkX_cuGraph\n",
            "\n",
            "Node 0 is connected to node 1.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Node 0 is connected to node 1.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the average degree of nodes?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Text_to_NetworkX_cuGraph (EZaLBoZYD)\n",
            " Call ID: EZaLBoZYD\n",
            "  Args:\n",
            "    query: What is the average degree of nodes?\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: Text_to_NetworkX_cuGraph\n",
            "\n",
            "Error: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
            " Please fix your mistakes.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the average degree of nodes?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Text_to_NetworkX_cuGraph (MYKGW1GAo)\n",
            " Call ID: MYKGW1GAo\n",
            "  Args:\n",
            "    query: What is the average degree of nodes?\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: Text_to_NetworkX_cuGraph\n",
            "\n",
            "EXEC ERROR: module 'networkx' has no attribute 'average_degree'\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The network is weakly connected. There are multiple connected components, and the largest component does not encompass a significant proportion of the total nodes, indicating a lack of strong overall connectivity. Node 0 is connected to node 1.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Who is connected to Node 0?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Text_to_NetworkX_cuGraph (BvwXM2PuT)\n",
            " Call ID: BvwXM2PuT\n",
            "  Args:\n",
            "    query: Who is connected to Node 0?\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: Text_to_NetworkX_cuGraph\n",
            "\n",
            "Error: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
            " Please fix your mistakes.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm sorry, I can't process your request at the moment.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Why?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm sorry, I can't process your request at the moment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MathBot(\"Who is connected to Node 0?\", [])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "pPtbk0exA9Ji",
        "outputId": "1ce63781-7807-44b3-8638-8db012080f78",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Text_to_AQL_to_Text` with `{'query': 'Who is connected to Node 0?'}`\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new ArangoGraphQAChain chain...\u001b[0m\n",
            "AQL Query (1):\u001b[32;1m\u001b[1;3mWITH Mathoverflow_node Mathoverflow_node_to_Mathoverflow_node MathOflow_node MathOflow_node_to_MathOflow_node Stackoverflow_node\n",
            "FOR vertex IN Mathoverflow_node\n",
            "  FILTER vertex._key == '0'\n",
            "  FOR edge IN 1..1 OUTBOUND vertex Mathoverflow_node_to_Mathoverflow_node\n",
            "    RETURN edge._to\n",
            "\u001b[0m\n",
            "AQL Result:\n",
            "\u001b[32;1m\u001b[1;3m[None, None, None, None, None, None, None, None, None, None]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mIt appears that Node 0 in the Mathoverflow graph does not have any outgoing connections. Therefore, there are no nodes directly connected to Node 0.\u001b[0m\u001b[32;1m\u001b[1;3m[{\"name\": \"Text_to_NetworkX_cuGraph\", \"arguments\": {\"query\": \"Who is connected to Node 0?\"}}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'append'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-40be75005834>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMathBot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Who is connected to Node 0?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-a0a7b5d47552>\u001b[0m in \u001b[0;36mMathBot\u001b[0;34m(query, history)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Append query history into the list (Definitely can change depending on what to pass to history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     query_history.append({\"Q_role\": \"user\" , \"Q_content\": final_state[\"input\"][0][\"content\"],\n\u001b[0m\u001b[1;32m     48\u001b[0m                          \"A_role\": \"AI\", \"A_content\": final_state[\"output\"]})\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dumpsite (Ignore from this point onwards)"
      ],
      "metadata": {
        "id": "CkkgVqLSr8t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ignore this lol\n",
        "# def yes(message, history):\n",
        "#     return \"yes\"\n",
        "\n",
        "# def vote(data: gr.LikeData):\n",
        "#     if data.liked:\n",
        "#         print(\"You upvoted this response: \" + data.value[\"value\"])\n",
        "#     else:\n",
        "#         print(\"You downvoted this response: \" + data.value[\"value\"])\n",
        "\n",
        "# # Use this to tell user to download/upload any saved caht file?\n",
        "\n",
        "# with gr.Blocks() as demo:\n",
        "#     chatbot = gr.Chatbot(placeholder=\"<strong>Your Personal Yes-Man</strong><br>Ask Me Anything\")\n",
        "#     chatbot.like(vote, None, None)\n",
        "#     gr.ChatInterface(fn=yes, type=\"messages\", chatbot=chatbot)\n",
        "\n",
        "# demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "collapsed": true,
        "id": "tcp2DOwV_W1z",
        "outputId": "601efca5-5505-425c-9d15-f0ad9f173712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:310: UserWarning: The type of the gr.Chatbot does not match the type of the gr.ChatInterface.The type of the gr.ChatInterface, 'messages', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b96bb57b8dd26535be.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b96bb57b8dd26535be.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OLD MATHBOT\n",
        "\n",
        "# def MathBot(query, history):\n",
        "#     global check_final_state\n",
        "#     global query_history\n",
        "\n",
        "#     # Create the agent\n",
        "#     llm = ChatMistralAI(model = \"mistral-small-latest\", temperature = 0.5, max_tokens = max_tokens, rate_limiter=rate_limiter,)\n",
        "#     agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "#     agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "#     # # Calculate the total tokens in conversation history\n",
        "#     # total_tokens = sum(llm.get_num_tokens(message[\"content\"]) for message in query_history)\n",
        "\n",
        "#     # # Remove oldest history if total tokens exceed the limit\n",
        "#     # while total_tokens > max_tokens:\n",
        "#     #   if len(query_history) > 2:\n",
        "#     #     removed_message = query_history.pop(0)\n",
        "#     #     total_tokens -= len(removed_message[\"content\"])\n",
        "#     #   else:\n",
        "#     #     break\n",
        "\n",
        "#     # Generate the output\n",
        "#     final_state = agent_executor.invoke({\"input\": [{\"role\": \"user\", \"content\": query}]})\n",
        "\n",
        "#     # Append query history into the list (Definitely can change depending on what to pass to history)\n",
        "#     query_history.append({\"Q_role\": \"user\" , \"Q_content\": final_state[\"input\"][0][\"content\"],\n",
        "#                          \"A_role\": \"AI\", \"A_content\": final_state[\"output\"]})\n",
        "\n",
        "#     # Save it into the user's query history file\n",
        "#     np.save('/content/user_query_history.npy', query_history)\n",
        "\n",
        "#     return final_state[\"output\"]"
      ],
      "metadata": {
        "id": "vK89k_vvr_o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Test = pd.DataFrame({\"Col1\": [1], \"Col2\": [4]})"
      ],
      "metadata": {
        "id": "UXnMfAR-pskm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsNcsixzrNSk",
        "outputId": "a0a6bdff-d8ce-4893-d23d-9b82e31abafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Col1    Col2    0\n",
            "0       1       4  NaN\n",
            "1  [2, 4]  [6, 6]  NaN\n",
            "2     NaN     NaN  2.0\n",
            "3     NaN     NaN  6.0\n",
            "4       2       6  NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Test = pd.concat([Test, pd.DataFrame([{\"Col1\": 2, \"Col2\": 6}])], ignore_index=True)"
      ],
      "metadata": {
        "id": "cBISjg0epveX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4-VpP8PpxvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code related to testing chat history/memory"
      ],
      "metadata": {
        "id": "x1TWF_T4EwKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Function for trimming the number of messages (Kinda shoddy) (Testing)\n",
        "# def Check_Trim():\n",
        "#     currMessages = agent.get_state(config).values[\"messages\"]\n",
        "\n",
        "#     # Start trimmin if there are more than 100 total messages\n",
        "#     if len(currMessages) > 1:\n",
        "\n",
        "#         # First message (index 0) is definitely HumanMessage (Earliest query)\n",
        "#         # Remove the earliest query and all the associated ToolMessage or AIMessage\n",
        "#         agent.update_state(config, {\"messages\": RemoveMessage(id=currMessages[0].id)})\n",
        "\n",
        "#         # For a valid list, the first message should be HumanMessage.\n",
        "#         # So remove all the responses until the next query (HumanMessage)\n",
        "#         while (agent.get_state(config).values[\"messages\"][0].type != \"human\"):\n",
        "\n",
        "#             currMessages = agent.get_state(config).values[\"messages\"]\n",
        "#             agent.update_state(config, {\"messages\": RemoveMessage(id=currMessages[0].id)})\n",
        "\n",
        "\n",
        "#     return \"Done trimming\""
      ],
      "metadata": {
        "id": "V5OSYWRT7b4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the length of the current query history\n",
        "# AIMessage, HumanMessage, and ToolMessage are separate\n",
        "\n",
        "# print(\"Current number of AIMessage + HumanMessages + Tool Message:\", len(agent.get_state(config).values[\"messages\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlV-g8yszuJf",
        "outputId": "8ac5c519-f5df-4603-f4a6-aef12eb6e704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current number of AIMessage + HumanMessages + Tool Message: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chat_history = agent.get_state(config).values[\"messages\"]\n",
        "# print(chat_history[0])#.type == \"human\")\n",
        "\n",
        "# for message in chat_history:\n",
        "#        message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "esRMzTm32tZk",
        "outputId": "b231465c-0206-4b2f-e8fa-6cf77eaac9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "My name is James.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello James! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#_ = agent.update_state(config, {\"messages\": [HumanMessage(\"Hey\")]})\n",
        "#_ = agent.update_state(config, {\"messages\": [AIMessage(\"Hello.\")]})"
      ],
      "metadata": {
        "id": "A04Jj-ky_IRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display(Image(agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t7gIFr1ecpzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"What is your favourite fruit?\"\n",
        "# query = \"Are there isolated nodes?\"\n",
        "# query = \"Which node has the highest betweenness centrality score? Use a k value of 10\"\n",
        "# query = \"Who is connected to Node 0?\"\n",
        "# query = \"What is the shortest path from Node 0 to Node 1?\"\n",
        "# query = \"Update node 0 to have attribute foo=bar\"\n",
        "# query = \"Is the graph fully connected?\"\n",
        "# query = \"What is the average degree of nodes?\"\n",
        "# query = \"Are there nodes that, if removed, would fragment the network?\"\n",
        "# query = \"Which users are outliers in terms of connections?\"\n",
        "# query = \"Which nodes are the most connected?\"\n",
        "# query = \"How strongly connected is the network? Used connected components\"\n",
        "# query = \"Fetch the highest pagerank node and value\"\n",
        "# query = \"Who is the most influential node?\"\n",
        "# query = \"Who is the most popular person in the Graph? Explain why\"\n",
        "\n",
        "input_message = (\n",
        "    \"My name is James.\"\n",
        ")\n",
        "\n",
        "for event in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
        "    stream_mode=\"values\",\n",
        "    config=config,\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MYZJzqqIdG22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6afb6449-986a-4234-f3c6-ec6b02f3d02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "My name is James.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello James! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For testing only, looks like work\n",
        "# def query_graph(query):\n",
        "#     final_state = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]}, config=config)\n",
        "#     return final_state[\"messages\"][-1].content"
      ],
      "metadata": {
        "id": "2XLEiKT-jSn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query_graph(\"What is the shortest path from Node 0 to Node 1?\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yBwr3_yBP03f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent 2 Probs won't use this(?)"
      ],
      "metadata": {
        "id": "-WUo0uk1zrJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LangGraph Implementation\n",
        "from typing import Literal\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import END, MessagesState, StateGraph\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "\n",
        "tools = [retrieve, text_to_aql_to_text, text_to_nx_algorithm_to_text]\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "llm = ChatMistralAI(model = \"mistral-small-latest\", temperature = 0.5, max_tokens = 1000, rate_limiter=rate_limiter,)\n",
        "tooled_llm = llm.bind_tools(tools)\n",
        "\n",
        "def should_continue(state: MessagesState):\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    if last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "    messages = state[\"messages\"]\n",
        "    time.sleep(0.1)\n",
        "    response = tooled_llm.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "workflow.add_edge(START, \"agent\")\n",
        "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "agent2 = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "uhTUyDIZxIlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_message = (\n",
        "    \"What's my name?\"\n",
        ")\n",
        "\n",
        "for event in agent2.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
        "    stream_mode=\"values\",\n",
        "    config=config,\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_N6W-zuyh1Y",
        "outputId": "6dfd2431-abad-4680-a296-838f35c60ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What's my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I don't have access to personal information about you. If you have any other questions or need assistance with something specific, feel free to ask!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the graph\n",
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(agent2.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "collapsed": true,
        "id": "XMQypmq0Z-Bo",
        "outputId": "9ce45d3c-c342-46ab-8168-c8756c451c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nothingburger"
      ],
      "metadata": {
        "id": "lWD1vyMDz0zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_core.messages import SystemMessage\n",
        "# from langgraph.checkpoint.memory import MemorySaver\n",
        "# from langgraph.graph import END, MessagesState, StateGraph\n",
        "# from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "# # Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
        "# def query_or_respond(state: MessagesState):\n",
        "#     \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
        "#     llm_with_tools = llm.bind_tools([retrieve, text_to_aql_to_text, text_to_nx_algorithm_to_text])\n",
        "#     time.sleep(0.1)\n",
        "#     response = llm_with_tools.invoke(state[\"messages\"])\n",
        "#     # MessagesState appends messages to state instead of overwriting\n",
        "#     return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# # Step 2: Execute the retrieval.\n",
        "# tools = ToolNode([retrieve, text_to_aql_to_text, text_to_nx_algorithm_to_text])\n",
        "\n",
        "\n",
        "# # Step 3: Generate a response using the retrieved content.\n",
        "# def generate(state: MessagesState):\n",
        "#     \"\"\"Generate answer.\"\"\"\n",
        "#     # Get generated ToolMessages\n",
        "#     recent_tool_messages = []\n",
        "#     for message in reversed(state[\"messages\"]):\n",
        "#         if message.type == \"tool\":\n",
        "#             recent_tool_messages.append(message)\n",
        "#         else:\n",
        "#             break\n",
        "#     tool_messages = recent_tool_messages[::-1]\n",
        "\n",
        "#     # Format into prompt\n",
        "#     docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
        "#     system_message_content = (\n",
        "#         \"You are an assistant for question-answering tasks. \"\n",
        "#         \"Use the following pieces of retrieved context to answer \"\n",
        "#         \"the question. If you don't know the answer, try to use other tools. Use three sentences maximum and keep the \"\n",
        "#         \"answer concise.\"\n",
        "#         \"\\n\\n\"\n",
        "#         f\"{docs_content}\"\n",
        "#     )\n",
        "#     conversation_messages = [\n",
        "#         message\n",
        "#         for message in state[\"messages\"]\n",
        "#         if message.type in (\"human\", \"system\")\n",
        "#         or (message.type == \"ai\" and not message.tool_calls)\n",
        "#     ]\n",
        "#     prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
        "\n",
        "#     # Run\n",
        "#     time.sleep(0.1)\n",
        "#     response = llm.invoke(prompt)\n",
        "\n",
        "#     return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# # Build graph\n",
        "# graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "# graph_builder.add_node(query_or_respond)\n",
        "# graph_builder.add_node(tools)\n",
        "# graph_builder.add_node(generate)\n",
        "\n",
        "# graph_builder.set_entry_point(\"query_or_respond\")\n",
        "# graph_builder.add_conditional_edges(\n",
        "#     \"query_or_respond\",\n",
        "#     tools_condition,\n",
        "#     {END: END, \"tools\": \"tools\"},\n",
        "# )\n",
        "\n",
        "# graph_builder.add_edge(\"tools\", \"generate\")\n",
        "# graph_builder.add_edge(\"generate\", END)\n",
        "\n",
        "# memory = MemorySaver()\n",
        "# graph = graph_builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "52Fig9-4cLxP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "L4nmLCFf3HoC",
        "0nWdOOAqXRXs",
        "nKd2Fn4bXRXs",
        "FDwAaBcsXRXt",
        "EyJQSd8cXRX0",
        "hh8FdF3UpDjv",
        "k6tikTOs4p7H",
        "-s557sEo0ENV",
        "aWRixZRhR8N-",
        "KF0wA_kGG0Wb",
        "BnNsyFPTpVrU",
        "CkkgVqLSr8t6",
        "lWD1vyMDz0zx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f35b46c3945d45de8df36bd217d13224": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f7a04313c37c400eb8958ce9a56319d3",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "(NX → ADB): Nodes \u001b[38;2;151;196;35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;151;196;35m╸\u001b[0m\u001b[38;5;237m━\u001b[0m \u001b[35m 97%\u001b[0m (320001/330317) \u001b[33m0:00:03\u001b[0m\n     ADB Import: 'Finefoods_node' (10000) \u001b[38;2;91;192;222m▰▰▰▰▱▱▱\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">(NX → ADB): Nodes <span style=\"color: #97c423; text-decoration-color: #97c423\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━</span> <span style=\"color: #800080; text-decoration-color: #800080\"> 97%</span> (320001/330317) <span style=\"color: #808000; text-decoration-color: #808000\">0:00:03</span>\n     ADB Import: 'Finefoods_node' (10000) <span style=\"color: #5bc0de; text-decoration-color: #5bc0de\">▰▰▰▰▱▱▱</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f7a04313c37c400eb8958ce9a56319d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e947e5e223574388bb94baf09134e02a": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2707febcf3164d1f9aa31893f80bf45a",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "(NX → ADB): Edges \u001b[38;2;94;49;8m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;94;49;8m╸\u001b[0m \u001b[35m100%\u001b[0m (560001/560804) \u001b[33m0:01:20\u001b[0m\n     ADB Import: 'Finefoods_node_to_Finefoods_node' (10000) \u001b[38;2;91;192;222m▰▰▰▰▰▰▱\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">(NX → ADB): Edges <span style=\"color: #5e3108; text-decoration-color: #5e3108\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> (560001/560804) <span style=\"color: #808000; text-decoration-color: #808000\">0:01:20</span>\n     ADB Import: 'Finefoods_node_to_Finefoods_node' (10000) <span style=\"color: #5bc0de; text-decoration-color: #5bc0de\">▰▰▰▰▰▰▱</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:01</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "2707febcf3164d1f9aa31893f80bf45a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}